{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f28d6",
   "metadata": {},
   "source": [
    "### Two ways of training the Linear Regression model\n",
    "- 1. Using the Normal Equation which leads directly to optimal weights \n",
    "- 2. Iterative way to compute Gradient Descent for optimal weight \n",
    "\n",
    "\n",
    "### Linear Regression Equation:\n",
    "$hθ​(x)=θ0​+θ1​x1​+θ2​x2​+⋯+θn​xn​$\n",
    "\n",
    "### And we trying to solve this equation:\n",
    "\n",
    "###### Mean Squared Error\n",
    "$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "\n",
    "##### Normal Equation:\n",
    "$ θ=(XTX)−1XTy$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94e2bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights: [1 2]\n",
      "initial bias: 0.3794767929747599\n",
      "normal equation weights:\n",
      " [[1.31473725]\n",
      " [2.37382172]]\n",
      "weights:\n",
      " [[1.01736876]\n",
      " [2.03929588]]\n",
      "bias: 0.3400888584441628\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(65)\n",
    "# lets create a dumy dataset \n",
    "X = np.random.rand(100, 2)\n",
    "w1 = np.array([1, 2])\n",
    "bias = np.random.rand(1)[0]\n",
    "y = X @ w1 + bias + np.random.randn(100)*0.1\n",
    "\n",
    "print(f\"initial weights: {w1}\")\n",
    "print(f\"initial bias: {bias}\")\n",
    "\n",
    "# after applying the normal equation\n",
    "weights = np.linalg.inv((X.T@X)) @ X.T @ y\n",
    "\n",
    "print(f\"normal equation weights:\\n {np.array(weights).reshape(-1, 1)}\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(f\"weights:\\n {np.array(lr.coef_).reshape(-1, 1)}\")\n",
    "print(f'bias: {lr.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58cccb",
   "metadata": {},
   "source": [
    "#### Using SVD in regression\n",
    "\n",
    "Let’s plug \n",
    "$X = U \\Sigma V^T$ \n",
    "\n",
    "$w = (X^T X)^{-1} X^T y$\n",
    "\n",
    "Substitute $X = U \\Sigma V^T$:\n",
    "\n",
    "$w = (V \\Sigma^T U^T U \\Sigma V^T)^{-1} V \\Sigma^T U^T y$\n",
    "\n",
    "Simplify using orthogonality $(U^T U = I and V^T V = I)$:\n",
    "\n",
    "$w = V \\Sigma^{-1} U^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "728bb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD-based weights: [1.31473725 2.37382172]\n"
     ]
    }
   ],
   "source": [
    "# SVD decomposition\n",
    "U, s, VT = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# Compute pseudo-inverse of Sigma\n",
    "Sigma_inv = np.diag(1 / s)\n",
    "\n",
    "# SVD-based weights\n",
    "w_svd = VT.T @ Sigma_inv @ U.T @ y\n",
    "\n",
    "print(\"SVD-based weights:\", w_svd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
